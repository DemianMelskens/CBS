{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "from instaloader import Post\n",
    "import pandas as pd\n",
    "import os\n",
    "import instaloader\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "filepath = '../../data/'\n",
    "filename = 'Coosto_berichten2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions    \n",
    "def outputCSV(dataset, filename):\n",
    "    dataset.to_csv(filepath + filename, sep=';')\n",
    "    \n",
    "def cprint(text):\n",
    "    sys.stdout.write(\"\\r\" + text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datum               22\n",
       "url                 22\n",
       "sentiment            6\n",
       "discussielengte     19\n",
       "views                5\n",
       "auteur              22\n",
       "GPS breedtegraad     2\n",
       "GPS lengtegraad      2\n",
       "bericht tekst       22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve data\n",
    "insta = pd.read_csv(filepath + filename, delimiter=';')\n",
    "\n",
    "# remove empty columns\n",
    "insta = insta.drop(['zoekopdracht', 'type', 'volgers', 'invloed', 'titel', 'type bron'], axis=1)\n",
    "\n",
    "# check with count()\n",
    "insta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all posts from instagram using an array of urls\n",
    "def get_posts(urls):\n",
    "    posts_dict = {}\n",
    "    total_length = len(urls)\n",
    "    \n",
    "    for index, url in enumerate(urls):\n",
    "        shortcode = url.split(\"/\")[-2]\n",
    "        \n",
    "        try:\n",
    "            L = instaloader.Instaloader()\n",
    "            post = Post.from_shortcode(L.context, url.split(\"/\")[-2])\n",
    "            posts_dict[shortcode] = post\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        cprint(\"Getting posts \" + str(round((index / total_length) * 100)) + \"% completed\")\n",
    "        \n",
    "    return posts_dict\n",
    "\n",
    "\n",
    "# Get the indexes of the posts which do not exist anymore\n",
    "def get_non_exsisting_posts(dataset, posts_dict):\n",
    "    indexes_to_drop = []\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        shortcode = row['url'].split(\"/\")[-2]\n",
    "        if not shortcode in posts_dict:\n",
    "            indexes_to_drop.append(index)\n",
    "    \n",
    "    return indexes_to_drop\n",
    "            \n",
    "\n",
    "# Delete posts from the dataset based on an array of indexes\n",
    "def del_posts(data, indexes_to_drop):\n",
    "    for index in indexes_to_drop:\n",
    "        data = data.drop(index=index, axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Enrich dataset with like count\n",
    "def add_like_count_to_dataset(dataset, posts_dict):\n",
    "    for index, row in dataset.iterrows():\n",
    "        shortcode = row['url'].split(\"/\")[-2]\n",
    "        if shortcode in posts_dict:\n",
    "            dataset.at[index, 'likes count'] = posts_dict[shortcode].likes\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Adds utc date to the dataset\n",
    "def add_date_utc(dataset, posts_dict):\n",
    "    for index, row in dataset.iterrows():\n",
    "        shortcode = row['url'].split('/')[-2]\n",
    "        if shortcode in posts_dict:\n",
    "            dataset.at[index, 'datum utc'] = posts_dict[shortcode].date_utc\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Refreshes comment count\n",
    "def refresh_comment_count(dataset, posts_dict):\n",
    "    for index, row in dataset.iterrows():\n",
    "        shortcode = row['url'].split('/')[-2]\n",
    "        if shortcode in posts_dict:\n",
    "            dataset.at[index, 'discussielengte'] = posts_dict[shortcode].comments\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Refreshes view count\n",
    "def refresh_views(dataset, posts_dict):\n",
    "    for index, row in dataset.iterrows():\n",
    "        shortcode = row['url'].split('/')[-2]\n",
    "        if shortcode in posts_dict:\n",
    "            dataset.at[index, 'views'] = posts_dict[shortcode].likes\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "# Cleans invalid urls and enriches with like count, data utc, comment count and view count\n",
    "def clean_und_enrich(dataset):\n",
    "    posts_dict = get_posts(dataset['url'])\n",
    "    indexes_to_drop = get_non_exsisting_posts(dataset, posts_dict)\n",
    "    dataset = del_posts(dataset, indexes_to_drop)\n",
    "    dataset = add_like_count_to_dataset(dataset, posts_dict)\n",
    "    dataset = add_date_utc(dataset, posts_dict)\n",
    "    dataset = refresh_comment_count(dataset, posts_dict)\n",
    "    dataset = refresh_views(dataset, posts_dict)\n",
    "    \n",
    "    cprint('\\nInvalid urls found: ' + str(len(indexes_to_drop)))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def improve_sentiment(dataset):\n",
    "    dataset['sentiment'] = dataset['sentiment'].replace(np.nan, '0')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def isolate_hashtag(data):  \n",
    "    total_hashtags = []\n",
    "    for index, row in data.iterrows():\n",
    "        text = row['bericht tekst'] \n",
    "        \n",
    "        # find all hashtags in text and isolate them in new column\n",
    "        total_hashtags.append(re.findall(r\"#(\\w+)\", text))\n",
    "        \n",
    "        #remove hashtags from text\n",
    "        pattern = re.compile(\"#(\\w+)\")\n",
    "        newText = pattern.sub(r'', text)\n",
    "        data.at[index, 'bericht tekst'] = newText\n",
    "        \n",
    "    data['hashtags'] = total_hashtags\n",
    "    return data\n",
    "\n",
    "def remove_emoji(data):  \n",
    "    indexes_to_drop = []\n",
    "        \n",
    "    for index, row in data.iterrows():\n",
    "        a = row['bericht tekst']\n",
    "\n",
    "        # todo: vul aan met meer emoji's\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        newValue = emoji_pattern.sub(r'', a)\n",
    "        newValue = newValue.replace('ðŸ¥—', '')\n",
    "        if newValue == '' : \n",
    "            indexes_to_drop.append(index)\n",
    "            \n",
    "        else:\n",
    "            data.at[index, 'bericht tekst'] = newValue\n",
    "    \n",
    "    pass\n",
    "    data = del_posts(data, indexes_to_drop)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting posts 95% completed\n",
      "Invalid urls found: 2"
     ]
    }
   ],
   "source": [
    "insta = improve_sentiment(insta)\n",
    "insta = remove_emoji(insta)\n",
    "insta = clean_und_enrich(insta)\n",
    "insta = isolate_hashtag(insta)\n",
    "\n",
    "# Resets index\n",
    "insta.index = range(len(insta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datum</th>\n",
       "      <th>url</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>discussielengte</th>\n",
       "      <th>views</th>\n",
       "      <th>auteur</th>\n",
       "      <th>GPS breedtegraad</th>\n",
       "      <th>GPS lengtegraad</th>\n",
       "      <th>bericht tekst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-29 14:45</td>\n",
       "      <td>https://instagram.com/p/Bg6FiptlWhz/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>533.0</td>\n",
       "      <td>sizezeroalkmaar</td>\n",
       "      <td>52.629131</td>\n",
       "      <td>4.749720</td>\n",
       "      <td>Je hebt het druk, maar wilt toch graag aan je ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 13:52</td>\n",
       "      <td>https://instagram.com/p/BdaFBieFJ8X/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>268.0</td>\n",
       "      <td>estherfeenstra</td>\n",
       "      <td>53.303810</td>\n",
       "      <td>5.052010</td>\n",
       "      <td>Gelukkig nieuwjaar! #bikkels #zijlieverdanik #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-28 13:15</td>\n",
       "      <td>https://instagram.com/p/BfvXGkyhiSf/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>someguyslivinginahouse</td>\n",
       "      <td>51.916698</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>Wandering around Rotterdam with the housemates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-19 19:15</td>\n",
       "      <td>https://instagram.com/p/BacAeTuAyyi/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.0</td>\n",
       "      <td>skinpractica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Skinpractica voor al uw huid en laserbehandeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-09-02 11:17</td>\n",
       "      <td>https://instagram.com/p/BYiIer7laoM/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>733.0</td>\n",
       "      <td>susannetendoesschate</td>\n",
       "      <td>51.986523</td>\n",
       "      <td>5.834437</td>\n",
       "      <td>Deze toppers van @2gettherecoaching lopen vand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datum                                   url sentiment  \\\n",
       "0  2018-03-29 14:45  https://instagram.com/p/Bg6FiptlWhz/       NaN   \n",
       "1  2018-01-01 13:52  https://instagram.com/p/BdaFBieFJ8X/       NaN   \n",
       "2  2018-02-28 13:15  https://instagram.com/p/BfvXGkyhiSf/       NaN   \n",
       "3  2017-10-19 19:15  https://instagram.com/p/BacAeTuAyyi/       NaN   \n",
       "4  2017-09-02 11:17  https://instagram.com/p/BYiIer7laoM/       NaN   \n",
       "\n",
       "   discussielengte  views                  auteur  GPS breedtegraad  \\\n",
       "0              NaN  533.0         sizezeroalkmaar         52.629131   \n",
       "1              NaN  268.0          estherfeenstra         53.303810   \n",
       "2              NaN    6.0  someguyslivinginahouse         51.916698   \n",
       "3              NaN   79.0            skinpractica               NaN   \n",
       "4              NaN  733.0    susannetendoesschate         51.986523   \n",
       "\n",
       "   GPS lengtegraad                                      bericht tekst  \n",
       "0         4.749720  Je hebt het druk, maar wilt toch graag aan je ...  \n",
       "1         5.052010  Gelukkig nieuwjaar! #bikkels #zijlieverdanik #...  \n",
       "2         4.500000  Wandering around Rotterdam with the housemates...  \n",
       "3              NaN  Skinpractica voor al uw huid en laserbehandeli...  \n",
       "4         5.834437  Deze toppers van @2gettherecoaching lopen vand...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output new cleaned dataset\n",
    "data = insta\n",
    "outputCSV(data, \"cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "chunksize = 2500\n",
    "made_files = 0\n",
    "\n",
    "def split_file(made_files):\n",
    "    for i,chunk in enumerate(pd.read_csv(filepath + filename, delimiter=';', chunksize=chunksize)):\n",
    "        chunk = chunk.drop(['zoekopdracht', 'type', 'volgers', 'invloed', 'titel', 'type bron'], axis=1)\n",
    "        chunk.to_csv(filepath + 'set{}.csv'.format(i), sep=';')\n",
    "        made_files +=1\n",
    "    return made_files\n",
    "\n",
    "made_files = split_file(made_files)\n",
    "print(made_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting posts 8% completed"
     ]
    }
   ],
   "source": [
    "#Cleaning al data files after another\n",
    "def clean_splitted_files():\n",
    "    for num in range(0, made_files):\n",
    "        setfile = pd.read_csv(filepath + \"set\"+ str(num) + '.csv', delimiter=';')\n",
    "\n",
    "        setfile = improve_sentiment(setfile)\n",
    "        setfile = remove_emoji(setfile)\n",
    "        setfile = clean_und_enrich(setfile)\n",
    "        setfile = isolate_hashtag(setfile)\n",
    "\n",
    "        # Resets index\n",
    "        outputCSV(setfile,\"set\"+ str(num) + '.csv')\n",
    "\n",
    "clean_splitted_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting posts 75% completed\n",
      "Getting posts 80% completed\n",
      "Getting posts 80% completed\n",
      "Getting posts 75% completed\n",
      "Getting posts 50% completed\n",
      "Invalid urls found: 0"
     ]
    }
   ],
   "source": [
    "#Mergin all all datafile in one file\n",
    "def merge_splitted_files_to_one:\n",
    "    merged_filename = \"merged_file.csv\"\n",
    "    try:\n",
    "        os.remove(filepath + merged_filename)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    outputfile = open(filepath + merged_filename, \"a\", encoding=\"utf8\")\n",
    "    for line in open(filepath + \"set0.csv\", encoding=\"utf8\"):\n",
    "        outputfile.write(line)\n",
    "    # now the rest:    \n",
    "    for num in range(1,made_files):\n",
    "        setfile = open(filepath +\"set\"+ str(num) +\".csv\", encoding=\"utf8\")\n",
    "        setfile.__next__() # skip the header\n",
    "        for line in setfile:\n",
    "             outputfile.write(line)\n",
    "        setfile.close() # not really needed\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
